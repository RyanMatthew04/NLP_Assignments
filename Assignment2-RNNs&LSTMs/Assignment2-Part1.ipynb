{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12977863,"sourceType":"datasetVersion","datasetId":8214217}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport requests\nimport zipfile\nimport os\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load and preprocess data\ndef load_and_preprocess_data(file_path):\n    df = pd.read_csv(file_path)\n    df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n    return df\n\n# Text preprocessing\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    return tokens\n\n# Build vocabulary\ndef build_vocab(texts, min_freq=2):\n    word_counts = Counter()\n    for text in texts:\n        word_counts.update(text)\n    \n    vocab = {'<PAD>': 0, '<UNK>': 1}\n    idx = 2\n    for word, count in word_counts.items():\n        if count >= min_freq:\n            vocab[word] = idx\n            idx += 1\n    return vocab\n\n# Download and load GloVe embeddings\ndef load_glove_embeddings(embedding_dim=100):\n    glove_path = f'glove.6B.{embedding_dim}d.txt'\n    \n    if not os.path.exists(glove_path):\n        print(\"Downloading GloVe embeddings...\")\n        url = f\"https://nlp.stanford.edu/data/glove.6B.zip\"\n        response = requests.get(url, stream=True)\n        with open('glove.6B.zip', 'wb') as f:\n            f.write(response.content)\n        \n        with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n            zip_ref.extractall('.')\n    \n    embeddings_index = {}\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    \n    return embeddings_index\n\n# Create embedding matrix\ndef create_embedding_matrix(vocab, embeddings_index, embedding_dim):\n    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n    for word, idx in vocab.items():\n        if word in embeddings_index:\n            embedding_matrix[idx] = embeddings_index[word]\n        else:\n            # Random initialization for unknown words\n            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n    return embedding_matrix\n\n# Dataset class\nclass MovieReviewDataset(Dataset):\n    def __init__(self, texts, labels, vocab, max_len=200):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Convert tokens to indices\n        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in text]\n        \n        # Pad or truncate\n        if len(indices) > self.max_len:\n            indices = indices[:self.max_len]\n        else:\n            indices = indices + [self.vocab['<PAD>']] * (self.max_len - len(indices))\n        \n        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:08:51.884868Z","iopub.execute_input":"2025-09-06T12:08:51.885115Z","iopub.status.idle":"2025-09-06T12:08:57.283350Z","shell.execute_reply.started":"2025-09-06T12:08:51.885088Z","shell.execute_reply":"2025-09-06T12:08:57.282587Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Vanilla RNN Model with GloVe\nclass GloVeRNN(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding_dim = embedding_matrix.shape[1]\n        self.vocab_size = embedding_matrix.shape[0]\n        \n        # Use pre-trained embeddings\n        self.embedding = nn.Embedding.from_pretrained(\n            torch.FloatTensor(embedding_matrix), \n            freeze=False, \n            padding_idx=0\n        )\n        \n        self.rnn = nn.RNN(\n            self.embedding_dim, \n            hidden_dim, \n            n_layers, \n            batch_first=True, \n            dropout=dropout\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)\n        hidden = self.dropout(hidden[-1])\n        return self.fc(hidden)\n\n# LSTM Model with GloVe\nclass GloVeLSTM(nn.Module):\n    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding_dim = embedding_matrix.shape[1]\n        self.vocab_size = embedding_matrix.shape[0]\n        \n        self.embedding = nn.Embedding.from_pretrained(\n            torch.FloatTensor(embedding_matrix), \n            freeze=False, \n            padding_idx=0\n        )\n        \n        self.lstm = nn.LSTM(\n            self.embedding_dim, \n            hidden_dim, \n            n_layers, \n            batch_first=True, \n            dropout=dropout,\n            bidirectional=False\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = self.dropout(hidden[-1])\n        return self.fc(hidden)\n\n# Vanilla RNN with torch.nn.Embedding (on-the-fly)\nclass TorchEmbedRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.rnn = nn.RNN(\n            embedding_dim, \n            hidden_dim, \n            n_layers, \n            batch_first=True, \n            dropout=dropout\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, hidden = self.rnn(embedded)\n        hidden = self.dropout(hidden[-1])\n        return self.fc(hidden)\n\n# LSTM with torch.nn.Embedding (on-the-fly)\nclass TorchEmbedLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(\n            embedding_dim, \n            hidden_dim, \n            n_layers, \n            batch_first=True, \n            dropout=dropout,\n            bidirectional=False\n        )\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = self.dropout(hidden[-1])\n        return self.fc(hidden)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:08:57.284598Z","iopub.execute_input":"2025-09-06T12:08:57.284971Z","iopub.status.idle":"2025-09-06T12:08:57.297004Z","shell.execute_reply.started":"2025-09-06T12:08:57.284951Z","shell.execute_reply":"2025-09-06T12:08:57.296370Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n    model = model.to(device)\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for batch in train_loader:\n            texts, labels = batch\n            texts, labels = texts.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            predictions = model(texts).squeeze(1)\n            loss = criterion(predictions, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            predicted = (torch.sigmoid(predictions) > 0.5).float()\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n        \n        # Validation\n        model.eval()\n        total_val_loss = 0\n        correct_val = 0\n        total_val = 0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                texts, labels = batch\n                texts, labels = texts.to(device), labels.to(device)\n                \n                predictions = model(texts).squeeze(1)\n                loss = criterion(predictions, labels)\n                total_val_loss += loss.item()\n                \n                predicted = (torch.sigmoid(predictions) > 0.5).float()\n                correct_val += (predicted == labels).sum().item()\n                total_val += labels.size(0)\n        \n        val_loss = total_val_loss / len(val_loader)\n        val_acc = correct_val / total_val\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{epochs}:')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        print('-' * 50)\n    \n    return train_losses, val_losses, train_accs, val_accs\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            texts, labels = batch\n            texts, labels = texts.to(device), labels.to(device)\n            \n            predictions = model(texts).squeeze(1)\n            predicted = (torch.sigmoid(predictions) > 0.5).float()\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    print(f'Test Accuracy: {accuracy:.4f}')\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:08:57.297573Z","iopub.execute_input":"2025-09-06T12:08:57.297788Z","iopub.status.idle":"2025-09-06T12:08:57.317774Z","shell.execute_reply.started":"2025-09-06T12:08:57.297773Z","shell.execute_reply":"2025-09-06T12:08:57.317143Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# Load and preprocess data\ndf = load_and_preprocess_data('/kaggle/input/imdb-dataset-nlp/IMDB Dataset.csv')\n\n# Preprocess texts\nprint(\"Preprocessing texts...\")\ndf['tokens'] = df['review'].apply(preprocess_text)\n\n# Split data\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Build vocabulary\nprint(\"Building vocabulary...\")\nvocab = build_vocab(train_df['tokens'])\n\n# Create datasets\ntrain_dataset = MovieReviewDataset(\n    train_df['tokens'].tolist(), \n    train_df['sentiment'].tolist(), \n    vocab\n)\nval_dataset = MovieReviewDataset(\n    val_df['tokens'].tolist(), \n    val_df['sentiment'].tolist(), \n    vocab\n)\ntest_dataset = MovieReviewDataset(\n    test_df['tokens'].tolist(), \n    test_df['sentiment'].tolist(), \n    vocab\n)\n    \n# Create data loaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Hyperparameters\nembedding_dim = 100\nhidden_dim = 256\noutput_dim = 1\nn_layers = 2\ndropout = 0.5\nlearning_rate = 0.001\nepochs = 5\n\n# Load GloVe embeddings\nprint(\"Loading GloVe embeddings...\")\nglove_embeddings = load_glove_embeddings(embedding_dim)\nembedding_matrix = create_embedding_matrix(vocab, glove_embeddings, embedding_dim)\n\n# Task 1: GloVe + Vanilla RNN\nprint(\"\\n\" + \"=\"*50)\nprint(\"TASK 1: GloVe + Vanilla RNN\")\nprint(\"=\"*50)\n\nmodel1 = GloVeRNN(embedding_matrix, hidden_dim, output_dim, n_layers, dropout)\noptimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\ncriterion = nn.BCEWithLogitsLoss()\n    \ntrain_losses1, val_losses1, train_accs1, val_accs1 = train_model(\n    model1, train_loader, val_loader, optimizer1, criterion, epochs\n)\ntest_acc1 = evaluate_model(model1, test_loader)\n\n# Task 2: GloVe + LSTM\nprint(\"\\n\" + \"=\"*50)\nprint(\"TASK 2: GloVe + LSTM\")\nprint(\"=\"*50)\n\nmodel2 = GloVeLSTM(embedding_matrix, hidden_dim, output_dim, n_layers, dropout)\noptimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n\ntrain_losses2, val_losses2, train_accs2, val_accs2 = train_model(\n    model2, train_loader, val_loader, optimizer2, criterion, epochs\n)\ntest_acc2 = evaluate_model(model2, test_loader)\n\n# Task 3: Torch Embedding + Vanilla RNN\nprint(\"\\n\" + \"=\"*50)\nprint(\"TASK 3: Torch Embedding + Vanilla RNN\")\nprint(\"=\"*50)\n\nmodel3 = TorchEmbedRNN(len(vocab), embedding_dim, hidden_dim, output_dim, n_layers, dropout)\noptimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n\ntrain_losses3, val_losses3, train_accs3, val_accs3 = train_model(\n    model3, train_loader, val_loader, optimizer3, criterion, epochs\n)\ntest_acc3 = evaluate_model(model3, test_loader)\n\n# Task 4: Torch Embedding + LSTM\nprint(\"\\n\" + \"=\"*50)\nprint(\"TASK 4: Torch Embedding + LSTM\")\nprint(\"=\"*50)\n\nmodel4 = TorchEmbedLSTM(len(vocab), embedding_dim, hidden_dim, output_dim, n_layers, dropout)\noptimizer4 = optim.Adam(model4.parameters(), lr=learning_rate)\n\ntrain_losses4, val_losses4, train_accs4, val_accs4 = train_model(\n    model4, train_loader, val_loader, optimizer4, criterion, epochs\n)\ntest_acc4 = evaluate_model(model4, test_loader)\n\n# Print final results\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL RESULTS COMPARISON\")\nprint(\"=\"*60)\nprint(f\"GloVe + RNN Test Accuracy: {test_acc1:.4f}\")\nprint(f\"GloVe + LSTM Test Accuracy: {test_acc2:.4f}\")\nprint(f\"Torch Embed + RNN Test Accuracy: {test_acc3:.4f}\")\nprint(f\"Torch Embed + LSTM Test Accuracy: {test_acc4:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-06T12:15:00.671814Z","iopub.execute_input":"2025-09-06T12:15:00.672076Z","iopub.status.idle":"2025-09-06T12:22:04.777591Z","shell.execute_reply.started":"2025-09-06T12:15:00.672059Z","shell.execute_reply":"2025-09-06T12:22:04.776774Z"}},"outputs":[{"name":"stdout","text":"Preprocessing texts...\nBuilding vocabulary...\nLoading GloVe embeddings...\n\n==================================================\nTASK 1: GloVe + Vanilla RNN\n==================================================\nEpoch 1/5:\nTrain Loss: 0.7060, Train Acc: 0.4999\nVal Loss: 0.6937, Val Acc: 0.4971\n--------------------------------------------------\nEpoch 2/5:\nTrain Loss: 0.6981, Train Acc: 0.5009\nVal Loss: 0.6926, Val Acc: 0.5117\n--------------------------------------------------\nEpoch 3/5:\nTrain Loss: 0.6976, Train Acc: 0.5030\nVal Loss: 0.6952, Val Acc: 0.5081\n--------------------------------------------------\nEpoch 4/5:\nTrain Loss: 0.6986, Train Acc: 0.5015\nVal Loss: 0.6938, Val Acc: 0.5081\n--------------------------------------------------\nEpoch 5/5:\nTrain Loss: 0.6980, Train Acc: 0.5029\nVal Loss: 0.6935, Val Acc: 0.4975\n--------------------------------------------------\nTest Accuracy: 0.5160\n\n==================================================\nTASK 2: GloVe + LSTM\n==================================================\nEpoch 1/5:\nTrain Loss: 0.6944, Train Acc: 0.5059\nVal Loss: 0.6930, Val Acc: 0.5081\n--------------------------------------------------\nEpoch 2/5:\nTrain Loss: 0.6934, Train Acc: 0.5093\nVal Loss: 0.6942, Val Acc: 0.4919\n--------------------------------------------------\nEpoch 3/5:\nTrain Loss: 0.7008, Train Acc: 0.5058\nVal Loss: 0.6946, Val Acc: 0.5081\n--------------------------------------------------\nEpoch 4/5:\nTrain Loss: 0.6966, Train Acc: 0.5049\nVal Loss: 0.6946, Val Acc: 0.4967\n--------------------------------------------------\nEpoch 5/5:\nTrain Loss: 0.6919, Train Acc: 0.5146\nVal Loss: 0.6813, Val Acc: 0.6164\n--------------------------------------------------\nTest Accuracy: 0.6247\n\n==================================================\nTASK 3: Torch Embedding + Vanilla RNN\n==================================================\nEpoch 1/5:\nTrain Loss: 0.7024, Train Acc: 0.5048\nVal Loss: 0.7022, Val Acc: 0.5120\n--------------------------------------------------\nEpoch 2/5:\nTrain Loss: 0.6996, Train Acc: 0.5017\nVal Loss: 0.6999, Val Acc: 0.5119\n--------------------------------------------------\nEpoch 3/5:\nTrain Loss: 0.6983, Train Acc: 0.4980\nVal Loss: 0.6945, Val Acc: 0.5113\n--------------------------------------------------\nEpoch 4/5:\nTrain Loss: 0.6986, Train Acc: 0.4999\nVal Loss: 0.6930, Val Acc: 0.5115\n--------------------------------------------------\nEpoch 5/5:\nTrain Loss: 0.6981, Train Acc: 0.5035\nVal Loss: 0.6936, Val Acc: 0.5113\n--------------------------------------------------\nTest Accuracy: 0.5159\n\n==================================================\nTASK 4: Torch Embedding + LSTM\n==================================================\nEpoch 1/5:\nTrain Loss: 0.6933, Train Acc: 0.5006\nVal Loss: 0.6931, Val Acc: 0.5099\n--------------------------------------------------\nEpoch 2/5:\nTrain Loss: 0.6856, Train Acc: 0.5316\nVal Loss: 0.6619, Val Acc: 0.6112\n--------------------------------------------------\nEpoch 3/5:\nTrain Loss: 0.6467, Train Acc: 0.6239\nVal Loss: 0.6700, Val Acc: 0.6277\n--------------------------------------------------\nEpoch 4/5:\nTrain Loss: 0.6502, Train Acc: 0.5990\nVal Loss: 0.6928, Val Acc: 0.5099\n--------------------------------------------------\nEpoch 5/5:\nTrain Loss: 0.6622, Train Acc: 0.5553\nVal Loss: 0.7224, Val Acc: 0.5013\n--------------------------------------------------\nTest Accuracy: 0.5160\n\n============================================================\nFINAL RESULTS COMPARISON\n============================================================\nGloVe + RNN Test Accuracy: 0.5160\nGloVe + LSTM Test Accuracy: 0.6247\nTorch Embed + RNN Test Accuracy: 0.5159\nTorch Embed + LSTM Test Accuracy: 0.5160\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}